
# nohup ~/.local/bin/snakemake --snakefile ../../Snakefile --cluster "sbatch --nice {cluster.oversubscribe} {cluster.gres}" --cluster-config ../cluster.json --cores 4222 -j 4222 -k --configfile  config.pt-en.yaml  &

import sys
import gzip
import lzma
import tldextract
import os
import os.path
import shutil
import subprocess
import mmh3
from tqdm import tqdm
from contextlib import contextmanager
from pathlib import Path
from toolwrapper import ToolWrapper
from func_timeout import func_timeout, FunctionTimedOut
from cerberus import Validator


###########################################################
# UTILS

def binary_available(cmd):
    cmd = "command -v " + cmd + " > /dev/null"
    callout = os.system(cmd)
    if callout == 0:
        return True
    else:
        return False

def tokeniser_check(cmd):
    proc = ToolWrapper(cmd.split())
    line = proc.writeline('test.test')
    try:
        tokline = func_timeout(5, proc.readline)
    except FunctionTimedOut:
        sys.stderr.write("ERROR: tokeniser could not complete within 5 seconds and was terminated. Is it buffering stdout? (if you are using Moses tokeniser, add -b)\n")
        exit(1)

def get_lang_or_default_from_dict(scripts_dict, language):
    script = ""
    if language in scripts_dict:
        script = scripts_dict[language]
    elif "default" in scripts_dict:
        script = scripts_dict["default"]

    return script

def get_customnbp_or_empty_from_dict(nbp_dict, language):
    nbp = ""
    if language in nbp_dict:
        nbp = nbp_dict[language]
    return nbp

def system_check(cmd):
    #sys.stderr.write("Executing:" + cmd + " on " + socket.gethostname() + "\n")
    #sys.stderr.flush()

    subprocess.check_call(cmd, shell=True)

@contextmanager
def open_gzip_or_plain(file_path):

    def decode_text(file_handler):
        for line in file_handler:
            yield line.decode('utf-8')

    f = None
    try:
        #print("file_path", file_path)
        if file_path[-3:] == ".gz":
            f = gzip.open(file_path, 'rb')
            yield decode_text(f)
        elif file_path[-3:] == ".xz":
            f = lzma.open(file_path, 'rb')
            yield decode_text(f)
        else:
            f = open(file_path, 'r')
            yield f

    except Exception as ex:
        sys.stderr.write(str(ex)+"\n")
        raise Exception("Error occured while loading a file {}".format(file_path))

    finally:
        if f:
            f.close()


def validate_args(config):
    schema = {
            'bitextor': {'required': True, 'type': 'string'},
            'lang1': {'required': True, 'type': 'string', 'maxlength': 2},
            'lang2': {'required': True, 'type': 'string', 'maxlength': 2},
            'temp': {'type': 'string'},
            'hunalignThreshold': {'type': 'float'},
            'maxlines': {'type': 'integer'},
            'parser': {'type': 'string', 'allowed': ['alcazar', 'bs4', 'modest', 'simple', 'extractcontent']},
            'onlyConcat': {'type': 'boolean'},
            'onlyPreprocessing': {'type': 'boolean'},
            'profiling': {'type': 'boolean'},
            'giawarc': {'type': 'boolean'},
            'preprocessLangs': {'type': 'string'},
            'ftfy': {'type': 'boolean'},
            'cleanHTML': {'type': 'boolean'},
            'langId': {'type': 'string', 'allowed': ['cld2', 'cld3']},

            'permanentDir': {'required': True, 'type': 'string'},
            'transientDir': {'required': True, 'type': 'string'},
            'dataDir': {'required': True, 'type': 'string'},
            'boilerpipeCleaning': {'type': 'boolean'},
            'pdf-converter' : {'type' : 'string'},
            'plainTextHashes' : {'type': 'string'},
            'httrack': {'type': 'boolean'}, #deprecated
            'crawler': {'type': 'string', 'allowed': ['wget', 'heritrix', 'creepy', 'httrack']},
            'crawlTld': {'type': 'boolean'},
            'crawlerNumThreads': {'type': 'integer'},
            'WARCFiles': {'type': 'list'},

            'dic': {'type': 'string'},
            'LANG2Detokenizer': {'type': 'string'},

            'sentenceSplitters': {'type': 'dict'},
            'wordTokenizers': {'type': 'dict'},
            'morphologicalAnalysers': {'type': 'dict'},
            'customNBPs': {'type': 'dict'},
            'pruneThreshold': {'type': 'integer'},
            'pruneType': {'type': 'string', 'allowed': ['words', 'chars']},

            'crawlerUserAgent': {'type': 'string'},
            'crawlSizeLimit': {'type': 'string'},
            'crawlTimeLimit': {'type': 'string'},
            'crawlWait': {'type': 'integer'},
            'crawlFileTypes': {'type': 'string'},
            'crawlPageLimit': {'type': 'integer'},
            'crawlerConnectionTimeout': {'type': 'integer'},
            'dumpCurrentCrawl': {'type': 'string'},
            'resumePreviousCrawl': {'type': 'string'},
            'heritrixPath': {'type': 'string'},
            'heritrixUrl': {'type': 'string'},
            'heritrixUser': {'type': 'string'},

            'documentAligner': {'type': 'string', 'allowed': ['DIC', 'externalMT', 'NMT', 'SMT']},
            'mosesDir': {'type': 'string'},
            'alignerCmd': {'type': 'string'},
            'bleualign': {'type': 'boolean'},
            'docAlignThreshold': {'type': 'float'},
            'bleuAlignThreshold': {'type': 'float'},
            'docAlignWorkers': {'type': 'integer'},

            'bicleaner': {'type': 'string'},
            'bicleanerThreshold': {'type': 'float'},
            'reverseOutputPair': {'type': 'boolean'},
            'elrc': {'type': 'boolean'},
            'bifixer': {'type': 'boolean'},
            'bifixerOptions': {'type': 'string'},
            'deferredCrawling': {'type': 'boolean'},
            'bicleanerCacheWithSents': {'type': 'boolean'},

            'deduped': {'type': 'boolean'},
            'hosts': {'type': 'list'},
            'hostsFile': {'type': 'string'},
            'excludeHosts': {'type': 'list'},
            'excludeHostsFile': {'type': 'string'},
            'linkedHosts': {'type': 'list'},
            'linkedHostsAction': {'type': 'string'},

            'langstat': {'type': 'string'},
            'langstatExcludeStrings': {'type': 'string'},
            'langstatThreshold': {'type': 'integer'},

            'initCorpusTrainPrefix': {'type': 'list'},
            'initCorpusDevPrefix': {'type': 'list'},
            'initCorpusTestPrefix': {'type': 'list'},
            'mkcls': {'type':'boolean'},
            'bicleanerCorpusTrainingPrefix': {'type': 'list'},

            'nmt': {'type': 'boolean'},
            'smt': {'type': 'boolean'},
            'storeRawCorpus': {'type': 'boolean'},
            'tmx': {'type': 'boolean'},

            'gpuId': {'type': 'integer'},
            'marianDir': {'type': 'string'},
            'marianArgs': {'type': 'list'},
            'marianModelFile': {'type': 'string'},
            'nmtVocabSize': {'type': 'integer'},

            'subwordNmtDir': {'type': 'string'},

            'mgiza': {'type': 'string'},

            'parallelBlock' : {'type': 'integer'},
            'parallelMemfree' : {'type': 'integer'},
            'parallelTranslate' : {'type': 'boolean'},

            }

    if ("httrack" in config and config["httrack"] == True) or ("crawler" in config and config["crawler"] == 'httrack'):
        if not binary_available("httrack"):
            sys.stderr.write("WARNING: HTTrack is not installed. Install it or disable option 'httrack' in the configuration file.\n")

    config.update({k: os.path.expanduser(v) if isinstance(v, str) else v for k, v in config.items()})
    config.update({k: [ os.path.expanduser(el) for el in v ] if 'Config' in k and v is list else v for k, v in config.items()})

    #Mandatory options depending on the document aligner method choosen
    if "documentAligner" in config:
        if config["documentAligner"]=='NMT':
            schema['marianDir']['required']=True
            schema['subwordNmtDir']['required']=True
            schema['mosesDir']['required']=True
            schema['LANG2Detokenizer']['required']=True
            schema['nmtVocabSize']['required']=True
            schema['gpuId']['required']=True
            schema['marianArgs']['required']=True
            schema['initCorpusTrainPrefix']['required']=True
            schema['initCorpusDevPrefix']['required']=True
            schema['initCorpusTestPrefix']['required']=True
        elif config["documentAligner"]=='SMT':
            schema['mosesDir']['required']=True
        elif config["documentAligner"]=='externalMT':
            schema['alignerCmd']['required']=True
        else:
            schema['dic']['required']=True
            if "dic" in config and not os.path.isfile(config["dic"]):
                sys.stderr.write("WARNING: dictionary {dic} was not found. Provide correct path to the dictionary or specify initCorpusTrainPrefix\n".format(dic=config["dic"]))
                schema['initCorpusTrainPrefix']['required']=True
    else:
        schema['dic']['required']=True
        if "dic" in config and not os.path.isfile(config["dic"]):
            schema['initCorpusTrainPrefix']['required']=True

    if "bicleaner" in config:
        if not os.path.isfile(config["bicleaner"]):
            schema['bicleanerCorpusTrainingPrefix']['required']=True

    if "linkedHosts" in config:
        schema['linkedHostsAction']['required']=True

    v = Validator(schema)
    #v.allow_unknown = True

    b = v.validate(config)

    if not b:
        print("Validation error. Stopping.", v.errors)
        exit()

###########################################################

validate_args(config)

#Local bitextor installation
BITEXTOR=config["bitextor"]

#Crawled languages
LANG1=config["lang1"]
LANG2=config["lang2"]

#Working paths
permanent=config["permanentDir"]
transient=config["transientDir"]
data=config["dataDir"]

PROFILING=""

if "reverseOutputPair" in config and config["reverseOutputPair"]:
    REVERSEOUTPUT = True
else:
    REVERSEOUTPUT = False

if REVERSEOUTPUT:
    LANG1REVERSE=LANG2
    LANG2REVERSE=LANG1
else:
    LANG1REVERSE=LANG1
    LANG2REVERSE=LANG2

if "giawarc" in config and config["giawarc"]:
    PPROC = "giawarc"
    FILES=["mime.gz", "url.gz"]
else:
    PPROC = "w2p"
    FILES=["deboilerplate_html.gz", "normalized_html.gz", "mime.gz", "url.gz"]

if "preprocessLangs" in config and config["preprocessLangs"]:
    PPROCLANGSOPT = "--langs " + config['preprocessLangs']
    PPROCLANGS = config['preprocessLangs'].split(',')
else:
    PPROCLANGSOPT = "--langs " + LANG1 + "," + LANG2
    PPROCLANGS = [LANG1, LANG2]

if "ftfy" in config and not config["ftfy"]:
    FTFY = ""
else:
    FTFY = "--ftfy"

if "cleanHTML" in config and config["cleanHTML"]:
    CLEANHTML = "--cleanhtml"
else:
    CLEANHTML = ""

if "langId" in config:
    LANGID = config["langId"]
else:
    LANGID = "cld2"

if "temp" in config:
  TMPDIR=config["temp"]
else:
  TMPDIR=transient
shell("mkdir -p "+TMPDIR)

if "hunalignThreshold" in config:
  MINQUALITY=config["hunalignThreshold"]
else:
  MINQUALITY=0.0

if "maxlines" in config:
  MAXLINES=config["maxlines"]
else:
  MAXLINES=-1

if "parser" in config and config["parser"]:
  PARSER = "--parser " + config["parser"]
else:
  PARSER = ""

if "profiling" in config and config["profiling"]:
  PROFILING="/usr/bin/time -v"

if "mkcls" in config and config["mkcls"]:
  MKCLS="/mgiza/mgizapp/inst/mkcls"
else:
  MKCLS="/clustercat/bin/mkcls"

system_check("mkdir -p " + permanent)
system_check("mkdir -p " + transient)

#Dictionary
if "dic" in config:
  DIC=config["dic"]
else:
  DIC=None

if "deduped" in config:
    DEDUP='--dedup "seg1,seg2"'
    BICLEANER_SORT="LC_ALL=C sort -t$'\t' -k3,4 -T "+TMPDIR+" --compress-program=gzip |"

#Option to remove Boilerpipe html: if the option is enabled, boilerpipe is not used
if "boilerpipeCleaning" in config and config["boilerpipeCleaning"]==True:
  boilerpipeCleaning = '--boilerpipe'
else:
  boilerpipeCleaning = ''

if "pdf-converter" in config and config["pdf-converter"]=="pdf-extract":
  usepdfextract = "--pdfextract"
else:
  usepdfextract = ""

if "plainTextHashes" in config:
  PLAINTEXTHASHES= "--input_hash "+config["plainTextHashes"]
else:
  PLAINTEXTHASHES=""

#Option to use HTTrack for crawling instead of the native bitextor crawler
if "httrack" in config and config["httrack"]==True:
  CRAWLTARGET="httrack"
else:
  CRAWLTARGET="creepy"

if "crawler" in config:
  CRAWLTARGET=config["crawler"]

#WARC files
if "WARCFiles" in config:
    WARCFILES=config["WARCFiles"]
else:
    WARCFILES=[]

WARCDOMAIN="WARCFILES-" + str(mmh3.hash(",".join(WARCFILES), signed=False))

#Tokenisers
if "sentenceSplitters" in config:
    SENTTOKS = config["sentenceSplitters"]
else:
    SENTTOKS = ""

if "wordTokenizers" in config:
    WORDTOKS = config["wordTokenizers"]
    WORDTOK1 = get_lang_or_default_from_dict(config["wordTokenizers"], LANG1)
    WORDTOK2 = get_lang_or_default_from_dict(config["wordTokenizers"], LANG2)
else:
    WORDTOKS = ""
    WORDTOK1, WORDTOK2 = BITEXTOR+'/preprocess/moses/tokenizer/tokenizer.perl -q -b -a -l ' + LANG1, BITEXTOR+'/preprocess/moses/tokenizer/tokenizer.perl -q -b -a -l ' + LANG2

if "morphologicalAnalysers" in config:
    MORPHTOKS = config["morphologicalAnalysers"]
    MORPHTOK1 = get_lang_or_default_from_dict(config["morphologicalAnalysers"], LANG1)
    MORPHTOK2 = get_lang_or_default_from_dict(config["morphologicalAnalysers"], LANG2)
else:
    MORPHTOKS = ""
    MORPHTOK1, MORPHTOK2 = "", ""

if "customNBPs" in config:
    CUSTOMNBPS = config["customNBPs"]
else:
    CUSTOMNBPS = ""


PRUNE_THRESHOLD = "--prune 80"
if "pruneThreshold" in config:
    PRUNE_THRESHOLD = "--prune " + str(config["pruneThreshold"])

PRUNE_TYPE = "--prune-type words"
if "pruneType" in config:
    PRUNE_TYPE = "--prune-type " + config["pruneType"]

############ OPTIONS FOR THE NATIVE BITEXTOR CRAWLER ############

#If this option is enabled the crawler will keep crawling across a whole top-level domain (.es, .com, .fr, etc.)
if "crawl-tld" in config and config["crawl-tld"]:
  TLD_CRAWL="-D"
else:
  TLD_CRAWL=""

#If this option is set, a specific user agent is used when crawling
if "crawlerUserAgent" in config:
  USERAGENT="-a \""+config["crawlerUserAgent"]+"\""
else:
  USERAGENT=""

#If this option is enabled, a size-limit is set for crawled data (for example "size-limit": "1G")
if "crawlSizeLimit" in config:
  CRAWLSIZELIMIT="-s "+config["crawlSizeLimit"]
else:
  CRAWLSIZELIMIT=""

#If this option is enabled, a time-limit is set for crawling data (for example "time-limit": "1h")
if "crawlTimeLimit" in config:
  CRAWLTIMELIMIT="-t "+str(config["crawlTimeLimit"])
else:
  CRAWLTIMELIMIT=""

if "crawlWait" in config:
  CRAWLWAIT="--wait "+str(config["crawlWait"])
else:
  CRAWLWAIT=""
if "crawlPageLimit" in config:
  CRAWLPAGELIMIT="-p "+str(config["crawlPageLimit"])
else:
  CRAWLPAGELIMIT=""

if "crawlFileTypes" in config:
  CRAWLFILETYPES="-f "+str(config["crawlFileTypes"])
else:
  CRAWLFILETYPES="-f html,pdf"

#Option to set how many threads will be used for crawling (default value: 2). Note that too many threads can cause the server hosting the website to reject some of the simultaneous connections.
if "crawlerNumThreads" in config:
  CRAWLJOBS="-j "+str(config["crawlerNumThreads"])
else:
  CRAWLJOBS="-j 2"

#Connection timeout in the crawler
if "crawlerConnectionTimeout" in config:
  CRAWLTIMEOUT="-o "+str(config["crawlerConnectionTimeout"])
else:
  CRAWLTIMEOUT=""

#If this option is set, the "crawler" object will be dump as a pickle, so crawling can be continued afterwards
if "dumpCurrentCrawl" in config:
  CRAWLDUMPARGS="-d "+config["dumpCurrentCrawl"]
else:
  CRAWLDUMPARGS=""

#If this option is set, crawling will be continued from the pickle object dumped in a previous crawl
if "resumePreviousCrawl" in config:
  CONTINUECRAWL="-l "+config["resumePreviousCrawl"]
else:
  CONTINUECRAWL=""

if "heritrixPath" in config:
  HERITRIXPATH=config["heritrixPath"]
else:
  HERITRIXPATH=""

if "heritrixUrl" in config:
  HERITRIXURL=config["heritrixUrl"]
else:
  HERITRIXURL="https://localhost:8443"

if "heritrixUser" in config:
  HERITRIXUSER=config["heritrixUser"]
else:
  HERITRIXUSER="admin:admin"

############ OPTIONS FOR THE MT DOCUMENT ALIGNER ############

#If documentAligner is enabled, Marek Střelec's MT-based document aligner is used (bitextor/document-aligner)
if "documentAligner" in config:
  if config["documentAligner"] == "externalMT":
    MT_COMMAND=config["alignerCmd"]
    DOCALIGNEXT="customMT"
    MTTYPE=DOCALIGNEXT
  elif config["documentAligner"] == "SMT":
    MT_COMMAND="smt"
    DOCALIGNEXT="smt"
    MTTYPE=DOCALIGNEXT
  elif config["documentAligner"] == "NMT":
    MT_COMMAND="nmt"
    DOCALIGNEXT="nmt"
    MTTYPE=DOCALIGNEXT
  else:
    MT_COMMAND=""
    DOCALIGNEXT="bitextor"
    MTTYPE=""
else:
  MT_COMMAND=""
  DOCALIGNEXT="bitextor"

if "docAlignWorkers" in config:
  DOCALIGNPARALLEL = "-j " + str(config["docAlignWorkers"])
else:
  DOCALIGNPARALLEL = ""

if "onlyPreprocessing" in config and config["onlyPreprocessing"]:
    DOCALIGNEXT="customMT"
    MT_COMMAND="cat -"

if "mosesDir" in config:
  MOSESDIR = config["mosesDir"]
else:
  MOSESDIR = ""

if "bleualign" in config and config["bleualign"]:
  SEGMENTALIGNER="bleualign"
else:
  SEGMENTALIGNER="hunalign"

# Marek says DOC_THRESHOLD~0.1, BLEU_THRESHOLD~ 0.1 - 0.3
if "docAlignThreshold" in config:
    DOC_THRESHOLD  = config["docAlignThreshold"]
else:
    DOC_THRESHOLD = 0.1

if "bleuAlignThreshold" in config:
    BLEU_THRESHOLD = config["bleuAlignThreshold"]
else:
    BLEU_THRESHOLD = 0.2
#print("DOC_THRESHOLD", DOC_THRESHOLD, "BLEU_THRESHOLD", BLEU_THRESHOLD)

############ FILTERING AND POST-PROCESSING OPTIONS ############
if "deferredCrawling" in config and config["deferredCrawling"]:
  DEFERREDFIELDS=",deferredseg1,checksum1,deferredseg2,checksum2"
  DEFERREDSENTENCES=".deferred"
else:
  DEFERREDFIELDS=""
  DEFERREDSENTENCES=""

if "bifixer" in config and config["bifixer"]:
  BIFIXER="bifixer"
  BIFIXERFIELD=",bifixerhash,bifixerscore"
  BICLEANER_SORT=""
  DEDUP='--dedup "bifixerhash"'
  if DEFERREDFIELDS != "":
      CACHEOPTIONS='-k 10'
  else:
      CACHEOPTIONS='-k 6'
else:
  BIFIXER="segclean"
  BIFIXERFIELD=""
  CACHEOPTIONS='-k 3,4'

if "bicleanerCacheWithSents" in config and config["bicleanerCacheWithSents"]:
  CACHEOPTIONS="-k 3,4"

if "bifixerOptions" in config:
    BIFIXEROPTIONS=config["bifixerOptions"]
else:
    BIFIXEROPTIONS="--aggressive_dedup"

if "bicleaner" in config:
  RAWOPTION="bicleaner.scores"
  BICLEANEROPTION=",bicleaner"
  BICLEANER="bicleaner"
  BICLEANER_CONFIG=config["bicleaner"]
  tokeniser_check(WORDTOK1)
  tokeniser_check(WORDTOK2)
else:
  RAWOPTION="segclean"
  BICLEANEROPTION=""
  BICLEANER=BIFIXER
  BICLEANER_CONFIG=""

if "bicleanerThreshold" in config:
  BICLEANER_THRESHOLD=config["bicleanerThreshold"]
else:
  BICLEANER_THRESHOLD=0.0

if "elrc" in config and config["elrc"]:
  ELRCSCORES="elrc"
  ELRCFIELDS=",lengthratio,numTokensSL,numTokensTL"
else:
  ELRCSCORES=BICLEANER
  ELRCFIELDS=""

###### MEMORY AND PARALLEL PROCESSING OPTIONS OPTIONS ############

# Set options for parallel command (for translation and bicleaner)
if "parallelBlock" in config and config["parallelBlock"]:
    PARALLEL_BLOCK=config["parallelBlock"]
else:
    PARALLEL_BLOCK=5000000

if "parallelMemfree" in config and config["parallelMemfree"]:
    PARALLEL_MEMFREE=config["parallelMemfree"]
else:
    PARALLEL_MEMFREE=20000000

if "parallelTranslate" in config and config["parallelTranslate"]:
    PARALLEL_TRANS_CMD="parallel -k --block {PARALLEL_BLOCK} --memfree {PARALLEL_MEMFREE} --pipe".format(PARALLEL_BLOCK=PARALLEL_BLOCK, PARALLEL_MEMFREE=PARALLEL_MEMFREE)
else:
    PARALLEL_TRANS_CMD=""

#========================= MAPPING URLS AND OUTPUT FILES =========================#

def create_domain_key_2_host_map(hosts):
    ret={}
    for host in hosts:
        # don't merge blog sites
        if host.find(".blogspot.") >= 0 or host.find(".wordpress.") >= 0:
           key = host
        else:
           key = tldextract.extract(host).domain

        if key not in ret:
            ret[key]=[]
        ret[key].append(host)
        #print("subdomain", key, host)
    return ret

def filter_tld(tlds):
    filtered_tlds={}
    if os.path.isfile("{permanent}/domains.gz".format(permanent=permanent)):
        with open_gzip_or_plain("{permanent}/domains.gz".format(permanent=permanent)) as f:
            for tld in f:
                tld=tld.strip()
                filtered_tlds[tld]=tlds[tld]
        return filtered_tlds
    else:
        return tlds

def init_concat_logic_link(domains):
    for tld,hosts in domains.items():
        if len(hosts) == 1 and  os.path.isfile("{data}/warc/{host}/{crawler}.warc.gz".format(data=data, host=hosts[0], crawler=CRAWLTARGET)) and not os.path.isfile("{data}/preprocess/{tld}/concat.warc.gz".format(tld=tld,data=data)):
            cmd="mkdir -p {data}/preprocess/{tld}; ln -sfn {data}/warc/{host}/{crawler}.warc.gz {data}/preprocess/{tld}/concat.warc.gz".format(data=data, host=hosts[0], crawler=CRAWLTARGET, transient=transient,
 tld=tld)
            shell(cmd)

def add_warc_files(domains, warcfiles):
    domains[WARCDOMAIN] = []
    for wf in warcfiles:
        if os.path.isfile(wf):
            domains[WARCDOMAIN].append(wf)
        else:
            print("WARC {path} omitted: file not found".format(path=wf))
    return domains

def init_warc_files_concat_logic(warcfiles):
    if len(warcfiles) == 1 and os.path.isfile(warcfiles[0]) and not os.path.isfile("{data}/preprocess/{domain}/concat.warc.gz".format(data=data, domain=WARCDOMAIN)):
        cmd="mkdir -p {data}/preprocess/{domain}; ln -sfn {warcpath} {data}/preprocess/{domain}/concat.warc.gz".format(data=data, warcpath=warcfiles[0], domain=WARCDOMAIN)
        shell(cmd)

def load_domains(file_path):
    domains = set()
    with file_path.open("r") as f:
        for line in f:
            line = line.strip()
            if len(line):
                domains.add(line)

    return domains

def get_domain_keys(hosts):
    keys = set()
    for host in hosts:
        domain = tldextract.extract(host).domain
        keys.add(domain)
    return keys

def exclude_hosts(hosts, excludeHosts):
    excludeKeys = get_domain_keys(excludeHosts)

    hostsCopy = set(hosts)
    print("BEFORE hosts", len(hosts), len(hostsCopy))

    for host in hostsCopy:
        key = tldextract.extract(host).domain
        if key in excludeKeys:
            hosts.remove(host)
    print("AFTER hosts", len(hosts), len(hostsCopy))

def get_hosts_from_langstat(langstat_path, lang1, lang2, threshold, exclude_path):
    print("langstat_path", langstat_path, file=sys.stderr)
    l12 = [lang1.lower(), lang2.lower()]

    excluded_set = set()
    if exclude_path:
        excluded_set = load_domains(Path(exclude_path))

    hostsToCrawl = set()

    #sys.stderr.write(
    #    "Gathering domain information for {0} and {1}...\n".format(*l12))
    with tqdm(total=None) as pbar:
        with open_gzip_or_plain(langstat_path) as f:

            prevHost = ""
            langContent = {}

            for line in f:
                split_line = line.strip().split()
                if len(split_line) != 3:
                    continue

                host, lang, byte_len = split_line
                name = tldextract.extract(host).domain
                #print("processing ", host, lang.lower(), byte_len, name)

                if host != prevHost:
                    # start of new host. Process previous entries
                    if len(langContent) == 2:
                        lang1_bytes = langContent[l12[0]]
                        lang2_bytes = langContent[l12[1]]
                        if lang1_bytes >= threshold and lang2_bytes >= threshold:
                            hostsToCrawl.add(prevHost)

                    prevHost = host
                    langContent = {}

                if lang.lower() in l12 and name not in excluded_set:
                    langContent[lang.lower()] = int(byte_len)

                pbar.update(1)

            # last host
            if len(langContent) == 2:
                lang1_bytes = langContent[l12[0]]
                lang2_bytes = langContent[l12[1]]
                if lang1_bytes >= threshold and lang2_bytes >= threshold:
                    hostsToCrawl.add(prevHost)

    return hostsToCrawl

def linked_hosts(permanent, dir, hosts, linkedHostsAction):
    #print("dir", dir)
    cmd = "mkdir -p " + permanent + "/warc"
    shell(cmd)

    with gzip.open(dir + "/hosts.gz", 'rt') as f:
        otherHosts = f.read().splitlines()

    otherHosts = set(otherHosts)
    #print("otherHosts", otherHosts)

    # make a copy in case we have to delete thing
    copyHosts = set(hosts)
    for host in copyHosts:
        if host in otherHosts:
            if linkedHostsAction == "remove" or linkedHostsAction == "postCrawlExclude":
                hosts.remove(host)
            elif linkedHostsAction == "link":
                dest = "{permanent}/warc/{host}".format(permanent=data, host=host)

                if not (os.path.exists(dest) or os.path.islink(dest)):
                    cmd = "ln -sfn {dir}/warc/{host} {dest}".format( host=host, dest=dest)
                    #print(cmd)
                    shell(cmd)
            else:
                sys.stderr.write("Unknown linkedHostsAction:" + linkedHostsAction + "\n")
                exit()

def post_crawl_exclude(hosts, postCrawlPath):
    with open(postCrawlPath, "rt") as f:
        excludeList = f.read().splitlines()
    for exclude in excludeList:
        exclude = exclude.strip()
        if len(exclude) > 0 and exclude in hosts:
            hosts.remove(exclude)

###############################################################################################
if os.path.isfile(permanent + "/hosts.gz"):
    with gzip.open(permanent + "/hosts.gz", 'rt') as f:
        hosts = f.read().splitlines()
    hosts = set(hosts)
    #sys.stderr.write("read hosts from file=" + str(len(hosts)) + "\n")

else:
    hosts = set()

    if "hosts" in config:
        newHosts = config["hosts"]
        hosts = hosts.union(newHosts)
        #sys.stderr.write("#hosts given=" + str(len(newHosts)) + "\n")

    if "langstat" in config:
        langstat_path = config["langstat"]
        lang1 = config["lang1"]
        lang2 = config["lang2"]
        threshold = int(config["langstatThreshold"])
        exclude_path = config["langstatExcludeStrings"]

        newHosts = get_hosts_from_langstat(langstat_path, lang1, lang2, threshold, exclude_path)
        hosts = hosts.union(newHosts)
        #sys.stderr.write("#hosts found in langstat=" + str(len(newHosts)) + "\n")

    if "hostsFile" in config:
        path = config["hostsFile"]
        with gzip.open(path, 'rt') as f:
            newHosts = f.read().splitlines()
            #sys.stderr.write("#hostsFile=" + str(len(newHosts)) + "\n")

        hosts = hosts.union(newHosts)

    if "excludeHosts" in config:
        excludeHosts = config["excludeHosts"]
        exclude_hosts(hosts, excludeHosts)

    if "excludeHostsFile" in config:
        with open(config["excludeHostsFile"], "rt") as f:
            excludeHosts = f.read().splitlines()
            print("excludeHosts", len(excludeHosts))
        exclude_hosts(hosts, excludeHosts)

    # if (len(hosts) == 0):
    #     print("No hosts found. Need at least one of hosts, langstat, hostsFile")
    #     exit()

    with gzip.open(permanent + "/hosts.gz", 'wt') as f:
        for host in hosts:
            f.write("%s\n" % host)

#sys.stderr.write("#hosts=" + str(len(hosts)) + "\n")

# exclude dead domains
postCrawlPath = "{permanent}/post-crawl-exclude".format(permanent=permanent)
#print("postCrawlPath", postCrawlPath)
if os.path.isfile(postCrawlPath):
    post_crawl_exclude(hosts, postCrawlPath)

if "linkedHosts" in config:
    linkedHostsAction = config["linkedHostsAction"]

    if linkedHostsAction != "postCrawlExclude":
        # exclude dead domains in other language pairs
        for dir in config["linkedHosts"]:
            postCrawlPath = "{dir}/permanent/post-crawl-exclude".format(dir=dir)
            #print("postCrawlPath", postCrawlPath)
            if os.path.isfile(postCrawlPath):
                post_crawl_exclude(hosts, postCrawlPath)

    for dir in config["linkedHosts"]:
        linked_hosts(data, dir, hosts, linkedHostsAction)

    if linkedHostsAction == "postCrawlExclude":
        # create list of dead domains in post-crawl-exclude file, only for this language pair
        print("Calc post-crawl exclude")
        #hostsPath = permanent + "/hosts.gz"
        #with gzip.open(hostsPath, 'rt') as f:
        #    hosts = f.read().splitlines()
        #hosts = set(hosts)
	#print("hosts", hosts)

        warcPath = permanent + "/warc"
        domainDirs = os.listdir(warcPath)
        for domainDir in domainDirs:
            #print(domainDir)
            warcFilePath = "{warcPath}/{domainDir}/httrack.warc.gz".format(warcPath=warcPath, domainDir=domainDir)
            if os.path.isfile(warcFilePath):
               hosts.remove(domainDir)
        #print("hosts", hosts)

        postCrawlPath = permanent + "/post-crawl-exclude"
        with open(postCrawlPath, 'wt') as f:
            f.write("\n".join(hosts))
        exit()

#sys.stderr.write("#hosts to crawl/process=" + str(len(hosts)) + "\n")
#exit(-1)

domainKey2Hosts = create_domain_key_2_host_map(hosts)
#If file domains.gz exists in the permanent directory, the dictionary domainKey2Hosts is filtered to contain only those TLD in this file
domainKey2Hosts = filter_tld(domainKey2Hosts)
#Function that checks if a domain has only one WARC and, if so, it creates a symbolic link
init_concat_logic_link(domainKey2Hosts)

if len(WARCFILES) != 0:
    #If WARC files are provided (through --WARCFiles option), add path to WARC file(s) to the domain WARCDOMAIN
    domainKey2Hosts = add_warc_files(domainKey2Hosts, WARCFILES)
    #If only one WARC is provided, creates a symbolic link to it
    init_warc_files_concat_logic(WARCFILES)

# print("domainKey2Hosts", domainKey2Hosts)


# every shell command will run sync
shell.prefix("sync; set -euo pipefail; ")
#shell.prefix("set -euo pipefail; ")

#================================== Creater Moses EMS config========================#
def create_moses_ems_config(workDir, mosesDir, mgiza, trainPrefixes, devPrefix, testPrefixes):
    with open("{mosesDir}/scripts/ems/example/config.basic.moses2".format(mosesDir=mosesDir), "r") as file:
        lines = file.read().split("\n")
        print("lines", len(lines))
        #print("lines", lines)

    lines[8] = "working-dir = {0}".format(workDir)
    lines[11] = "input-extension = " + LANG1
    lines[12] = "output-extension = " + LANG2
    lines[13] = "#" + lines[13]

    lines[18] = "moses-src-dir = {mosesDir}".format(mosesDir=mosesDir)
    lines[27] = "external-bin-dir = {mgiza}/mgizapp/inst".format(mgiza=mgiza)
    lines[79] = "jobs = 10"
    lines[94] = "cores = 8"
    lines[146] = "settings = \"--prune '0 0 1' -T $working-dir/lm -S 20% --discount_fallback\" "
    lines[310] = "training-options = \"-mgiza -mgiza-cpus 8\""
    lines[384] = "binarize-all = $moses-script-dir/training/binarize-model.perl"

    # DELETES
    lines[30] = ""
    lines[33] = ""
    lines[36] = ""
    lines[39] = ""
    lines[39] = ""

    # corpus
    lines[132] = lines[132] + " IGNORE"
    lines[133] = lines[133] + " IGNORE"

    # lm
    lines[206] = lines[206] + " IGNORE"
    lines[207] = lines[207] + " IGNORE"

    # tuning
    lines[522] = ""
    lines[528] = ""

    # eval
    lines[640] = "#" + lines[640]
    lines[641] = "#" + lines[641]
    lines[645] = 'sacre-bleu = "sacrebleu -lc"'
    lines[646] = 'sacre-bleu-c = "sacrebleu"'
    lines[664] = "#" + lines[664]
    lines[670] = "#" + lines[670]

    lines[682] = lines[682] + " IGNORE"

    # ADD
    # eval
    lines.insert(710, "")

    line = 711
    for path in testPrefixes:
        name = os.path.basename(path)
        lines.insert(line, "[EVALUATION:{name}]".format(name=name))
        lines.insert(line + 1, "raw-input = {path}.{lang}".format(path=path, lang=LANG1))
        lines.insert(line + 2, "raw-reference = {path}.{lang}".format(path=path, lang=LANG2))

        line += 3

    # tuning
    assert(len(devPrefix) == 1)
    lines[523] = "raw-input = {path}.{lang}".format(path=devPrefix[0], lang=LANG1)
    lines[529] = "raw-reference = {path}.{lang}".format(path=devPrefix[0], lang=LANG2)

    # lm
    line = 215
    for path in trainPrefixes:
        name = os.path.basename(path)
        lines.insert(line, "[LM:{name}]".format(name=name))
        lines.insert(line + 1, "raw-corpus = {path}.{lang}".format(path=path, lang=LANG2))

        line += 2

    # corpus
    lines.insert(137, "")

    line = 138
    for path in trainPrefixes:
        name = os.path.basename(path)
        lines.insert(line, "[CORPUS:{name}]".format(name=name))
        lines.insert(line + 1, "raw-stem = {path}".format(path=path))

        line += 2


    with open("{0}/steps/1/config.1".format(workDir), "w") as file:
        file.write("\n".join(lines))

#================================== START SNAKEMAKE================================#

#================================== TARGET FILES ==================================#

OUTPUT=[]
PPROCOUTPUT=[]

if "onlyConcat" in config and config["onlyConcat"]:
    for tld in domainKey2Hosts.keys():
        OUTPUT.append("{dir}/preprocess/{tld}/concat.warc.gz".format(dir=data, tld=tld))
else:
    for tld in domainKey2Hosts.keys():
        for lang in PPROCLANGS:
            for file in FILES + ["plain_tokenized.gz", "plain_sentences.gz"]:
                PPROCOUTPUT.append("{dir}/preprocess/{tld}/{pproc}/{lang}/{file}".format(dir=data, tld=tld, pproc=PPROC, lang=lang, file=file))

    if "onlyPreprocessing" not in config or not config["onlyPreprocessing"]:
        OUTPUT.append("{dir}/{l1}-{l2}.sent.xz".format(dir=permanent, l1=LANG1REVERSE, l2=LANG2REVERSE))
        if "nmt" in config and config["nmt"]:
            OUTPUT.append("{dir}/nmt-dir/evaluation/report".format(dir=transient))
            OUTPUT.append("{dir}/nmt-dir-crawl/evaluation/report".format(dir=transient))

        if "smt" in config and config["smt"]:
            OUTPUT.append("{dir}/smt-dir/steps/1/REPORTING_report.1.DONE".format(dir=transient))
            OUTPUT.append("{dir}/smt-dir-crawl/steps/1/REPORTING_report.1.DONE".format(dir=transient))

        #Optional TMX: if option enabled, TMX is generated
        if "tmx" in config and config["tmx"]:
            if "storeRawCorpus" in config and config["storeRawCorpus"]:
                OUTPUT.append("{dir}/{l1}-{l2}.raw.xz".format(dir=permanent, l1=LANG1REVERSE, l2=LANG2REVERSE))
            if "deduped" in config and config["deduped"]:
                OUTPUT.append("{dir}/{l1}-{l2}.deduped.tmx.xz".format(dir=permanent, l1=LANG1REVERSE, l2=LANG2REVERSE))
            else:
                OUTPUT.append("{dir}/{l1}-{l2}.not-deduped.tmx.xz".format(dir=permanent, l1=LANG1REVERSE, l2=LANG2REVERSE))

rule all:
    input:
        expand("{preprocess}", preprocess=PPROCOUTPUT),
        expand("{target}", target=OUTPUT)

#================================== SMT ======================================#

rule train_smt_with_crawl_data:
    input:
        l1="{dir}/crawl.{lang}".format(dir=permanent, lang=LANG1)
        ,
        l2="{dir}/crawl.{lang}".format(dir=permanent, lang=LANG2)
        ,
        emsConfig = "{dir}/smt-dir-crawl/steps/1/config.1".format(dir=transient)

    output:
        report = "{dir}/smt-dir-crawl/steps/1/REPORTING_report.1.DONE".format(dir=transient)
    run:
        cmd = "cd {transient}/smt-dir-crawl &&  {MOSESDIR}/scripts/ems/experiment.perl --continue 1 --exec;"
        shell(cmd)

rule train_smt_with_crawl_data_create_config:
    input:
        l1="{dir}/crawl.{lang}".format(dir=permanent, lang=LANG1)
        ,
        l2="{dir}/crawl.{lang}".format(dir=permanent, lang=LANG2)
        ,
        config = "{dir}/config.json".format(dir=transient)

    output:
        emsConfig = "{dir}/smt-dir-crawl/steps/1/config.1".format(dir=transient)
    priority: 40

    run:
        cmd = "mkdir -p {dir}/smt-dir-crawl/steps/1".format(dir=transient)
        shell(cmd)

        trainData = config["initCorpusTrainPrefix"]

        crawlPref = "{dir}/crawl".format(dir=permanent)
        trainData.append(crawlPref)

        create_moses_ems_config("{0}/smt-dir-crawl".format(transient),
                                MOSESDIR, config["mgiza"],
                                trainData,
                                config["initCorpusDevPrefix"],
                                config["initCorpusTestPrefix"])

rule train_smt_all:
    input:
        emsConfig = "{dir}/smt-dir/steps/1/config.1".format(dir=transient)

    output:
        report = "{dir}/smt-dir/steps/1/REPORTING_report.1.DONE".format(dir=transient)
    run:
        cmd = "cd {transient}/smt-dir && {MOSESDIR}/scripts/ems/experiment.perl --continue 1 --exec;"
        shell(cmd)

rule train_smt_all_create_config:
    input:
        config = "{dir}/config.json".format(dir=transient)

    output:
        emsConfig = "{dir}/smt-dir/steps/1/config.1".format(dir=transient)
    priority: 40

    run:
        cmd = "mkdir -p {dir}/smt-dir/steps/1".format(dir=transient)
        shell(cmd)

        create_moses_ems_config("{0}/smt-dir".format(transient),
                                MOSESDIR, config["mgiza"],
                                config["initCorpusTrainPrefix"],
                                config["initCorpusDevPrefix"],
                                config["initCorpusTestPrefix"])

#================================== NMT ======================================#


rule train_nmt_with_crawl_data:
    input:
        l1="{dir}/crawl.{lang}".format(dir=permanent, lang=LANG1)
        ,
        l2="{dir}/crawl.{lang}".format(dir=permanent, lang=LANG2)
        ,
        config = "{dir}/config.json".format(dir=transient)

    output:
        report = "{dir}/nmt-dir-crawl/evaluation/report".format(dir=transient)

    priority: 50
    run:
        trainData = config["initCorpusTrainPrefix"]

        crawlPref = "{dir}/crawl".format(dir=permanent)
        trainData.append(crawlPref)

        cmd = "snakemake --snakefile {BITEXTOR}/snakemake/nmt/Snakefile --configfile {input.config} -k -j3" \
            + " --directory {transient}/nmt-dir-crawl" \
            + " --config initCorpusTrainPrefix=\"" + str(trainData) + "\"" \
            + " permanentDir={permanent}/nmt-dir-crawl"
        print("cmd", cmd)
        shell(cmd)

rule train_nmt_all:
    input:
        config = "{transient}/config.json"

    output:
        report = "{transient}/nmt-dir/evaluation/report"

    priority: 50
    run:
        cmd = "snakemake --snakefile {BITEXTOR}/snakemake/nmt/Snakefile --configfile {input.config} -k -j3" \
            + " --directory {transient}/nmt-dir" \
            + " --config permanentDir={permanent}/nmt-dir"
        shell(cmd)

rule create_config:
    output:
        config = temp("{dir}/config.json".format(dir=transient))

    priority: 50
    run:
        with open(output.config, "wt") as configFile:
            configFile.write(str(config))


#================================== CRAWLING ======================================#
rule creepy_download:
    params:
        url="http://{target}"
    output:
        '{dir}/warc/{{target}}/creepy.warc.gz'.format(dir=data)
    priority: 10
    shell:
        #'echo {params.url}; '
        '{PROFILING} python3 {BITEXTOR}/bitextor-creepy.py {TLD_CRAWL} {CRAWLSIZELIMIT} {CRAWLTIMELIMIT} {CRAWLWAIT} {CRAWLJOBS} {CRAWLTIMEOUT} {CRAWLDUMPARGS} {CONTINUECRAWL} {USERAGENT} {params.url} > {output}'

rule httrack_download:
    output:
        '{dir}/warc/{{target}}/httrack.warc.gz'.format(dir=data)
    params:
        url="http://{target}"
    priority: 10
    shell:
        'echo hostname=$HOSTNAME; '
        #'mkdir -p {permanent}/warc/{wildcards.target} ; '
        'DIRNAME=$(mktemp -d {TMPDIR}/downloaded.{wildcards.target}.XXXXXX); '
        '{PROFILING} nice ionice -c 3 {BITEXTOR}/bitextor-httrack.py --url {params.url} --output-path $DIRNAME {CRAWLTIMELIMIT} {CRAWLPAGELIMIT} {USERAGENT} {CRAWLWAIT}; '
        '{PROFILING} nice ionice -c 3 {BITEXTOR}/bitextor-webdir2warc.sh $DIRNAME > {output}; '
        'rm -rf $DIRNAME;'

rule wget_download:
    output:
        '{dir}/warc/{{target}}/wget.warc.gz'.format(dir=data)
    params:
        url="http://{target}"
    priority: 10
    shell:
        'echo hostname=$HOSTNAME; '
        'DIRNAME=$(mktemp -d "{TMPDIR}/downloaded.{wildcards.target}.XXXXXX"); '
        '{PROFILING} nice ionice -c 3 {BITEXTOR}/bitextor-wget.py --url {params.url} --output-path "$DIRNAME" {CRAWLTIMELIMIT} {USERAGENT} {CRAWLFILETYPES} {CRAWLWAIT} --warc {output}; '
        'rm -rf $DIRNAME;'

rule heritrix_download:
    output:
        '{dir}/warc/{{target}}/heritrix.warc.gz'.format(dir=data)
    params:
        url="http://{target}"
    priority: 10
    shell:
        'echo hostname=$HOSTNAME; '
        'if [ "$(ps aux | grep -i Heritrix | grep -v grep)" == "" ] ; then {HERITRIXPATH}/bin/heritrix -a {HERITRIXUSER} ; fi ; '
        'curl -v -d "action=teardown" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}; '
        'curl -v -d "createpath={wildcards.target}&action=create" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine; '
        'DIRNAME=$(mktemp -d "{TMPDIR}/downloaded.{wildcards.target}.XXXXXX"); '
        'cat {BITEXTOR}/crawler-beans.cxml | sed "s@http://example.example/example@{params.url}@g" > $DIRNAME/my-crawler-beans.cxml; '
        'curl -v -T $DIRNAME/my-crawler-beans.cxml -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}/jobdir/crawler-beans.cxml; '
        'curl -v -d "action=build" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}; '
        'curl -v -d "action=launch&checkpoint=latest" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}; '
        'sleep 2; curl -v -d "action=unpause" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}; '
        'echo "Waiting for the warc.gz to be finished"; '
        'RUNTIME=0; '
        'sleep 15;'
        'while [ -f {HERITRIXPATH}/jobs/{wildcards.target}/latest/warcs/*warc.gz.open ] ; do echo "Waiting 5 seconds..." ; sleep 5 ; RUNTIME=$((RUNTIME+5)) ; if [ "{CRAWLTIMELIMIT}" != "" ] ; then if [ $RUNTIME -gt "{CRAWLTIMELIMIT}" ] ; then echo "Crawling time limit reached" ; curl -v -d "action=pause" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}; curl -v -d "action=checkpoint" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}; curl -v -d "action=terminate" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}; fi ; fi; done; '
        'echo "Job {wildcards.target} finished!"; '
        'cat {HERITRIXPATH}/jobs/{wildcards.target}/*/warcs/*warc.gz > {output}; '


def get_domain_hosts(wildcards):
    output=[]
    for h in domainKey2Hosts[wildcards.target]:
        if wildcards.target == WARCDOMAIN: # host is a WARC
            output.append(h)
        else:
            output.append('{dir}/warc/{host}/{crawler}.warc.gz'.format(dir=data, host=h, crawler=CRAWLTARGET))
    return output

rule concat_subdomains:
    input:
        get_domain_hosts
    output:
        "{dir}/preprocess/{{target}}/concat.warc.gz".format(dir=data)
    priority: 9
    run:
        assert(len(input))
        if len(input) == 1:
            cmd = "ln -sfn {input} {output}; "
        else:
            cmd = 'cat {input} > {output}; '
        shell(cmd)

#================================== PREPROCESSING ======================================#
# Preprocessing using giawarc with 'bitextorlang' format using GZip compression
# Writes output in {data}/preprocess/{domain}/giawarc
rule giawarc:
    input:
        '{data}/preprocess/{{domain}}/concat.warc.gz'.format(data=data)
    output:
        plain=temp(expand('{data}/preprocess/{{domain}}/giawarc/{lang}/plain_text.gz', data=data, lang=PPROCLANGS)),
        files=expand("{data}/preprocess/{{domain}}/giawarc/{lang}/{file}", data=data, lang=PPROCLANGS, file=FILES),
        hash='{data}/preprocess/{{domain}}/giawarc/plain_text_hashes.xz'.format(data=data)
    params:
        folder="{data}/preprocess/{{domain}}/giawarc".format(data=data)
    priority: 8
    threads: 2
    shell:
        '''
        mkdir -p {params.folder}
        {PROFILING} {BITEXTOR}/bitextor-warc2htmlwarc.py {CLEANHTML} {FTFY} --input {input} {usepdfextract} |
        {PROFILING} nice ionice -c 3 ~/go/bin/giawarc -f bilang -o {params.folder} -l {LANGID} --output_hash {output.hash} {PLAINTEXTHASHES} -
        for lang in {PPROCLANGS}; do
          if [ ! -f {params.folder}/$lang/plain_text.gz ]
            then >&2 echo "WARNING: no \'$lang\' data found in {wildcards.domain}. Creating empty files instead"
            mkdir -p {params.folder}/$lang
            touch {params.folder}/$lang/plain_text {params.folder}/$lang/mime {params.folder}/$lang/url
            gzip {params.folder}/$lang/*
          fi
        done
	'''

# Preprocssing using bitextor-warc2preprocess with 'bitextorlang' format with GZip compression
# Writes output in {data}/preprocess/{domain}/w2p
rule warc2preprocess:
    input:
        '{data}/preprocess/{{domain}}/concat.warc.gz'.format(data=data)
    output:
        hash='{data}/preprocess/{{domain}}/w2p/plain_text_hashes.xz'.format(data=data),
        plain=expand('{data}/preprocess/{{domain}}/w2p/{lang}/plain_text.gz', data=data, lang=PPROCLANGS),
	#plain=temp(expand('{data}/preprocess/{{domain}}/w2p/{lang}/plain_text.gz', data=data, lang=PPROCLANGS)),
        files=expand('{data}/preprocess/{{domain}}/w2p/{lang}/{file}', data=data, lang=PPROCLANGS, file=FILES)
    params:
        folder='{data}/preprocess/{{domain}}/w2p'.format(data=data)
    priority: 8
    threads: 2
    shell:
        '''
        mkdir -p {params.folder}
        {PROFILING} {BITEXTOR}/bitextor-warc2htmlwarc.py {CLEANHTML} {FTFY} --input {input} {usepdfextract} --disable-output-gzip |
        {PROFILING} nice ionice -c 3 {BITEXTOR}/bitextor-warc2preprocess.py --input - {PPROCLANGSOPT} --lang1 {LANG1} --lang2 {LANG2} --compression gz {boilerpipeCleaning} --langid {LANGID} --output-dir {params.folder} --output_hash {output.hash} {PLAINTEXTHASHES} {PARSER}
        for lang in {PPROCLANGS}; do
          if [ ! -f {params.folder}/$lang/plain_text.gz ]
            then >&2 echo "WARNING: no \'$lang\' data found in {wildcards.domain}. Creating empty files instead"
            mkdir -p {params.folder}/$lang
            touch {params.folder}/$lang/plain_text {params.folder}/$lang/mime {params.folder}/$lang/url {params.folder}/$lang/normalized_html {params.folder}/$lang/deboilerplate_html
            gzip {params.folder}/$lang/{{plain_text,mime,url,normalized_html,deboilerplate_html}}
          fi
        done
        '''

# tokenize every bitextorlang/{lang}/plain_text.gz to obtain bitextorlang/{lang}/plain_tokenized.gz
rule tokenize:
    input: '{dir}/{lang}/plain_text.gz'
    params:
        splitter=lambda wildcards: get_lang_or_default_from_dict(SENTTOKS, wildcards.lang),
        tokenizer=lambda wildcards: get_lang_or_default_from_dict(WORDTOKS, wildcards.lang),
        lemmatizer=lambda wildcards: get_lang_or_default_from_dict(MORPHTOKS, wildcards.lang),
        customnbp=lambda wildcards: get_customnbp_or_empty_from_dict(CUSTOMNBPS, wildcards.lang)
    output:
        tok='{dir}/{lang}/plain_tokenized.gz',
        sent='{dir}/{lang}/plain_sentences.gz'
    shell:
        '{PROFILING} {BITEXTOR}/bitextor-tokenize.py --text {input} --sentence-splitter "{params.splitter}" --word-tokenizer "{params.tokenizer}" --morph-analyser "{params.lemmatizer}" --langcode "{wildcards.lang}" --customnbp "{params.customnbp}" --sentences-output {output.sent} --tokenized-output {output.tok} {PRUNE_THRESHOLD} {PRUNE_TYPE};'

#================================== DICTIONARY-BASED DOCUMENT ALIGNMENT ==================================#
rule lettr2idx:
    input:
        text1='{data}/preprocess/{{target}}/{pproc}/{lang}/plain_tokenized.gz'.format(data=data,pproc=PPROC,lang=LANG1),
        text2='{data}/preprocess/{{target}}/{pproc}/{lang}/plain_tokenized.gz'.format(data=data,pproc=PPROC,lang=LANG2),
    output:
        '{transient}/{{target}}/idx.xz'.format(transient=transient)
    shell:
        '{PROFILING} {BITEXTOR}/bitextor-buildidx.py  --lang1 {LANG1} --lang2 {LANG2} -m 15 --text1 {input.text1} --text2 {input.text2} | xz -T 0 > {output}'

rule idx2ridx_l1tol2:
    input:
        '{dir}/idx.xz',
        expand("{dic}", dic=DIC)
    output:
        '{dir}/1.ridx.xz'
    shell:
        'xzcat -T 0 -f {input[0]} | {PROFILING} {BITEXTOR}/bitextor-idx2ridx.py -d {input[1]} --lang1 {LANG1} --lang2 {LANG2} | xz -T 0 > {output}'

rule idx2ridx_l2tol1:
    input:
        '{dir}/idx.xz',
        expand("{dic}", dic=DIC)
    output:
        '{dir}/2.ridx.xz'
    shell:
        'xzcat -T 0 -f {input[0]} | {PROFILING} {BITEXTOR}/bitextor-idx2ridx.py -d {input[1]} --lang1 {LANG2} --lang2 {LANG1} | xz -T 0 > {output}'

rule ridx2imagesetoverlap:
    input:
        '{transient}/{{target}}/{{num}}.ridx.xz'.format(transient=transient),
        '{data}/preprocess/{{target}}/{pproc}/{LANG1}/deboilerplate_html.gz'.format(data=data,pproc=PPROC,LANG1=LANG1),
        '{data}/preprocess/{{target}}/{pproc}/{LANG2}/deboilerplate_html.gz'.format(data=data,pproc=PPROC,LANG2=LANG2)
    output:
        '{transient}'.format(transient=transient)+'/{target}/{num}.imgoverlap.xz'
    shell:
        'xzcat -T 0 -f {input[0]} | {PROFILING} {BITEXTOR}/features/bitextor-imagesetoverlap.py --html1 {input[1]} --html2 {input[2]} | xz -T 0 > {output}'

rule imagesetoverlap2structuredistance:
    input:
        '{transient}/{{target}}/{{num}}.imgoverlap.xz'.format(transient=transient),
        '{data}/preprocess/{{target}}/{pproc}/{LANG1}/deboilerplate_html.gz'.format(data=data, pproc=PPROC,LANG1=LANG1),
        '{data}/preprocess/{{target}}/{pproc}/{LANG2}/deboilerplate_html.gz'.format(data=data, pproc=PPROC,LANG2=LANG2)
    output:
        '{transient}/{{target}}/{{num}}.structuredistance.xz'.format(transient=transient)
    shell:
        'xzcat -T 0 -f {input[0]} | {PROFILING} {BITEXTOR}/features/bitextor-structuredistance.py --html1 {input[1]} --html2 {input[2]} | xz -T 0 > {output}'

rule structuredistance2urldistance:
    input:
        '{transient}/{{target}}/{{num}}.structuredistance.xz'.format(transient=transient),
        '{data}/preprocess/{{target}}/{pproc}/{LANG1}/deboilerplate_html.gz'.format(data=data, pproc=PPROC,LANG1=LANG1),
        '{data}/preprocess/{{target}}/{pproc}/{LANG2}/deboilerplate_html.gz'.format(data=data, pproc=PPROC,LANG2=LANG2),
        '{data}/preprocess/{{target}}/{pproc}/{LANG1}/url.gz'.format(data=data, pproc=PPROC,LANG1=LANG1),
        '{data}/preprocess/{{target}}/{pproc}/{LANG2}/url.gz'.format(data=data, pproc=PPROC,LANG2=LANG2)

    output:
        '{transient}/{{target}}/{{num}}.urldistance.xz'.format(transient=transient)
    priority: 8
    shell:
        'xzcat -T 0 -f {input[0]} |{PROFILING} {BITEXTOR}/features/bitextor-urlsdistance.py --html1 {input[1]} --html2 {input[2]} --url1 {input[3]} --url2 {input[4]} | xz -T 0 > {output}'

rule urldistance2mutuallylinked:
    input:
        '{transient}/{{target}}/{{num}}.urldistance.xz'.format(transient=transient),
        '{data}/preprocess/{{target}}/{pproc}/{LANG1}/deboilerplate_html.gz'.format(data=data, pproc=PPROC,LANG1=LANG1),
        '{data}/preprocess/{{target}}/{pproc}/{LANG2}/deboilerplate_html.gz'.format(data=data, pproc=PPROC,LANG2=LANG2),
        '{data}/preprocess/{{target}}/{pproc}/{LANG1}/url.gz'.format(data=data, pproc=PPROC,LANG1=LANG1),
        '{data}/preprocess/{{target}}/{pproc}/{LANG2}/url.gz'.format(data=data, pproc=PPROC,LANG2=LANG2)
    output:
        '{transient}/{{target}}/{{num}}.mutuallylinked.xz'.format(transient=transient)
    shell:
        'xzcat -T 0 -f {input[0]} | {PROFILING} {BITEXTOR}/features/bitextor-mutuallylinked.py --html1 {input[1]} --html2 {input[2]} --url1 {input[3]} --url2 {input[4]} | xz -T 0 > {output}'

rule mutuallylinked2urlscomparison:
    input:
        '{transient}/{{target}}/{{num}}.mutuallylinked.xz'.format(transient=transient),
        '{data}/preprocess/{{target}}/{pproc}/{LANG1}/url.gz'.format(data=data, pproc=PPROC,LANG1=LANG1),
        '{data}/preprocess/{{target}}/{pproc}/{LANG2}/url.gz'.format(data=data, pproc=PPROC,LANG2=LANG2)
    output:
        '{transient}/{{target}}/{{num}}.urlscomparison.xz'.format(transient=transient)
    shell:
        'xzcat -T 0 -f {input[0]} | {PROFILING} {BITEXTOR}/features/bitextor-urlscomparison.py --url1 {input[1]} --url2 {input[2]} | xz -T 0 > {output}'

rule urlscomparison2urlsoverlap:
    input:
        '{transient}/{{target}}/{{num}}.urlscomparison.xz'.format(transient=transient),
        '{data}/preprocess/{{target}}/{pproc}/{LANG1}/deboilerplate_html.gz'.format(data=data, pproc=PPROC,LANG1=LANG1),
        '{data}/preprocess/{{target}}/{pproc}/{LANG2}/deboilerplate_html.gz'.format(data=data, pproc=PPROC,LANG2=LANG2)
    output:
        '{transient}'.format(transient=transient)+'/{target}/{num}.urlsoverlap.xz'
    shell:
        'xzcat -T 0 -f {input[0]} | {PROFILING} {BITEXTOR}/features/bitextor-urlsetoverlap.py --html1 {input[1]} --html2 {input[2]} | xz -T 0 > {output}'

rule urlsoverlap2rank:
    input:
        '{dir}/{num}.urlsoverlap.xz'
    output:
        '{dir}/{num}.rank.xz'
    shell:
        'xzcat -T 0 -f {input[0]} | {PROFILING} {BITEXTOR}/bitextor-rank.py -m {BITEXTOR}/model/keras.model -w {BITEXTOR}/model/keras.weights | xz -T 0 > {output}'

rule aligndocumentsBitextor:
    input:
        '{transient}/{{target}}/1.rank.xz'.format(transient=transient),
        '{transient}/{{target}}/2.rank.xz'.format(transient=transient),
        '{data}/preprocess/{{target}}/{pproc}/{LANG1}/url.gz'.format(data=data, pproc=PPROC,LANG1=LANG1),
        '{data}/preprocess/{{target}}/{pproc}/{LANG2}/url.gz'.format(data=data, pproc=PPROC,LANG2=LANG2)
    output:
        '{transient}/{{target}}/{l1}-{l2}.bitextor.matches'.format(transient=transient, l1=LANG1, l2=LANG2)
    shell:
        '{PROFILING} {BITEXTOR}/bitextor-align-documents.py --lines1 $(zcat {input[2]} | wc -l) --lines2 $(zcat {input[3]} | wc -l) -n 1 -i converge -r /dev/null {input[0]} {input[1]} > {output}'

################# MT-BASED DOCUMENT ALIGNMENT #################

rule docalign_extracted:
    input:
        '{data}/preprocess/{{target}}/{pproc}/{{lang}}/plain_sentences.gz'.format(data=data, pproc=PPROC)
    output:
        '{transient}/{{target}}/docalign/{{lang}}.extracted.xz'.format(transient=transient)
    params:
        docalign_folder='{transient}/{{target}}/docalign'.format(transient=transient)
    shell:
        'mkdir -p {params.docalign_folder};'
        'zcat {input} | {PROFILING} {BITEXTOR}/document-aligner/utils/extract_lett.py | xz -T 0 -c > {output}'

rule docalign_translate_nmt:
    input:
        source="{transient}/{{domain}}/docalign/{l1}.extracted.xz".format(transient=transient, l1=LANG1),
        target="{data}/preprocess/{{domain}}/{pproc}/{l2}/plain_sentences.gz".format(data=data, pproc=PPROC, l2=LANG2),
        configFile = "{transient}/config.json".format(transient=transient),
        report = "{transient}/nmt-dir/evaluation/report".format(transient=transient)
    output:
        "{transient}/{{domain}}/docalign/{l1}.nmt.extracted.translated.xz".format(transient=transient, l1=LANG1)
    priority: 40
    shell:
        # if  l1 content or l2 content are empty, skip translation (output empty file instead)
        '''
        if [ $(xzcat {input.source} | head -n 5 | wc -c) -eq 0 ] || [ $(zcat {input.target} | head -n 1 | wc -c) -eq 0 ]; then
           >&2 echo "EARLY STOPPING: Discontinuing domain {wildcards.domain}, as {LANG1} or {LANG2} not found"
           touch {output}.empty && xz {output}.empty && mv {output}.empty.xz {output}
        else
           xzcat -T 0 {input.source} | cut -f2 |
           {PROFILING} {BITEXTOR}/preprocess/bin/cache {PARALLEL_TRANS_CMD} {BITEXTOR}/snakemake/nmt/translate.sh {input.configFile} {transient}/{wildcards.domain}/docalign {permanent}/nmt-dir |
           paste <(xzcat -T 0 {input.source} | cut -f1) - |
           xz -c -T 0 > {output}
           if [ "$(xzcat -T 0 {input.source} | cut -f 2 | wc -l)" -ne "$(xzcat -T 0 {output} | cut -f 2 | wc -l)" ]; then
             >&2 echo "TRANSLATION ERROR (command {MT_COMMAND}): {input.source} and {output} should have the same number of lines"
             exit 2
           fi
        fi
        '''

rule docalign_translate_smt:
    input:
        source="{transient}/{{domain}}/docalign/{l1}.extracted.xz".format(transient=transient, l1=LANG1),
        target="{data}/preprocess/{{domain}}/{pproc}/{l2}/plain_sentences.gz".format(data=data, pproc=PPROC, l2=LANG2),
        report = "{transient}/smt-dir/steps/1/REPORTING_report.1.DONE".format(transient=transient)
    output:
        "{transient}/{{domain}}/docalign/{l1}.smt.extracted.translated.xz".format(transient=transient, l1=LANG1)
    params:
        smtDir = "{0}/smt-dir".format(config["transientDir"])
    shell:
        # if  l1 content or l2 content are empty, skip translation (output empty file instead)
        '''
        if [ $(xzcat {input.source} | head -n 5 | wc -c) -eq 0 ] || [ $(zcat {input.target} | head -n 5 | wc -c) -eq 0 ]; then
           >&2 echo "EARLY STOPPING: Discontinuing domain {wildcards.domain}, as {LANG1} or {LANG2} not found"
           touch {output}.empty && xz {output}.empty && mv {output}.empty.xz {output}
        else
           xzcat -T 0 {input.source} | cut -f2 |
           {PROFILING} {BITEXTOR}/preprocess/bin/cache {PARALLEL_TRANS_CMD} {BITEXTOR}/snakemake/translate-smt.sh {LANG1} {MOSESDIR} {params.smtDir} |
           paste <(xzcat -T 0 {input.source} | cut -f1) - |
           xz -c -T 0 > {output}
           if [ "$(xzcat -T 0 {input.source} | cut -f 2 | wc -l)" -ne "$(xzcat -T 0 {output} | cut -f 2 | wc -l)" ]; then
             >&2 echo "TRANSLATION ERROR (command {MT_COMMAND}): {input.source} and {output} should have the same number of lines"
             exit 2
           fi
        fi
        '''

rule docalign_custom_translate:
    input:
        source="{transient}/{{domain}}/docalign/{l1}.extracted.xz".format(transient=transient, l1=LANG1),
        target="{data}/preprocess/{{domain}}/{pproc}/{l2}/plain_sentences.gz".format(data=data, pproc=PPROC, l2=LANG2),
    output:
        "{transient}/{{domain}}/docalign/{l1}.customMT.extracted.translated.xz".format(transient=transient, l1=LANG1)
    shell:
        # if  l1 content or l2 content are empty, skip translation (output empty file instead)
        '''
        if [ $(xzcat {input.source} | head -n 5 | wc -c) -eq 0 ] || [ $(zcat {input.target} | head -n 5 | wc -c) -eq 0 ]; then
           >&2 echo "EARLY STOPPING: Discontinuing domain {wildcards.domain}, as {LANG1} or {LANG2} not found"
           touch {output}.empty && xz {output}.empty && mv {output}.empty.xz {output}
        else
           xzcat -T 0 {input.source} | cut -f2 |
           {PROFILING} {BITEXTOR}/preprocess/bin/cache {PARALLEL_TRANS_CMD} {MT_COMMAND} |
           paste <(xzcat -T 0 {input.source} | cut -f1) - |
           xz -c -T 0 > {output}
           if [ "$(xzcat -T 0 {input.source} | cut -f 2 | wc -l)" -ne "$(xzcat -T 0 {output} | cut -f 2| wc -l)" ]; then
             >&2 echo "TRANSLATION ERROR (command {MT_COMMAND}): {input.source} and {output} should have the same number of lines"
             exit 2
             fi
        fi
        '''

rule translated2base64:
    input:
        "{dir}/{prefix}.extracted.translated.xz"
    output:
        "{dir}/{prefix}.translated_sentences.xz"
    shell:
        "xzcat {input} | {PROFILING} {BITEXTOR}/document-aligner/utils/extracted2base64.py | xz -T 0 -c > {output}"

rule docalign_tokenize_translated:
    input:
        translated="{{dir}}/docalign/{l1}.{{mttype}}.extracted.translated.xz".format(l1=LANG1)
    output:
        "{{dir}}/docalign/{l1}.{{mttype}}.extracted.translated.tokenized.xz".format(l1=LANG1)
    shell:
        '''
        if [ -z "{MORPHTOK2}" ]; then
          xzcat -T 0 {input.translated} | cut -f 2 |
          {PROFILING} {WORDTOK2} | awk \'{{print tolower($0)}}\' |
          paste <(xzcat -T 0 {input.translated} | cut -f 1) - |
          xz -T 0 > {output}
        else
          xzcat -T 0 {input.translated} | cut -f 2 |
          {PROFILING} {WORDTOK2} | {PROFILING} {MORPHTOK2} | awk \'{{print tolower($0)}}\' |
          paste <(xzcat -T 0 {input.translated} | cut -f 1) - |
          xz -T 0 > {output}
        fi
        '''

rule translatedtokenized2base64:
    input:
        "{dir}/{prefix}.extracted.translated.tokenized.xz"
    output:
        "{dir}/{prefix}.translated_tokenized.xz"
    shell:
        "xzcat {input} | {PROFILING} {BITEXTOR}/document-aligner/utils/extracted2base64.py | xz -T 0 -c > {output}"

rule docalign_matches:
    input:
        l1="{transient}/{{target}}/docalign/{l1}.{{mttype}}.translated_tokenized.xz".format(transient=transient,l1=LANG1),
        l2="{data}/preprocess/{{target}}/{pproc}/{l2}/plain_tokenized.gz".format(data=data, pproc=PPROC, l2=LANG2)
    output:
        "{transient}/{{target}}/{l1}-{l2}.{{mttype}}.matches".format(transient=transient, l1=LANG1,l2=LANG2)
    shell:
        "{PROFILING} {BITEXTOR}/document-aligner/bin/docalign {input.l1} {input.l2} --threshold {DOC_THRESHOLD} {DOCALIGNPARALLEL} > {output}"

#================================== SEGMENT ALIGNMENT ==================================#

################# BLEUALIGN RULES #################

rule matches2bleualign:
    input:
        indices='{transient}/{{target}}/{l1}-{l2}.{{docalignext}}.matches'.format(transient=transient, l1=LANG1, l2=LANG2),
        plain1='{data}/preprocess/{{target}}/{pproc}/{lang}/plain_sentences.gz'.format(data=data, pproc=PPROC, lang=LANG1),
        plain2='{data}/preprocess/{{target}}/{pproc}/{lang}/plain_sentences.gz'.format(data=data, pproc=PPROC, lang=LANG2),
        url1='{data}/preprocess/{{target}}/{pproc}/{lang}/url.gz'.format(data=data, pproc=PPROC, lang=LANG1),
        url2='{data}/preprocess/{{target}}/{pproc}/{lang}/url.gz'.format(data=data, pproc=PPROC, lang=LANG2),
        translated1='{transient}/{{target}}/docalign/{l1}.{{docalignext}}.translated_sentences.xz'.format(transient=transient,l1=LANG1),
    output: '{transient}/{{target}}/bleualign.docalign.{{docalignext}}.xz'.format(transient=transient)
    params:
        c1=lambda wildcards: 1 if wildcards.docalignext == 'bitextor' else 2,
        c2=lambda wildcards: 2 if wildcards.docalignext == 'bitextor' else 3
    shell:
        '''
        cut -f {params.c1},{params.c2} {input.indices} |
        LC_ALL=C sort -nk1 |
        {PROFILING} python3 {BITEXTOR}/bitextor-build-docalign.py --columns1 {input.url1} {input.plain1} {input.translated1} --columns2 {input.url2} {input.plain2} |
        awk -F\'\t\' \'{{print $2,$6,$3,$7,$4}}\' OFS=\'\t\' |
        xz -T 0 -f > {output} # Format: url1 <tab> url2 <tab> text1 <tab> text2 <tab> text1translated
        '''

rule alignsegments_bleualign:
    input:
        "{{dir}}/bleualign.docalign.{extension}.xz".format(extension=DOCALIGNEXT)
    output:
        "{dir}/bleualign.segalign.xz"
    shell:
        "xzcat {input} | {PROFILING} {BITEXTOR}/bleualign-cpp/bleualign_cpp --bleu-threshold {BLEU_THRESHOLD} | xz -T 0 -c > {output}"

################# HUNALIGN RULES #################

rule matches2hunalign:
    input:
        indices='{transient}/{{target}}/{l1}-{l2}.{{docalignext}}.matches'.format(transient=transient, l1=LANG1, l2=LANG2),
        plain1='{data}/preprocess/{{target}}/{pproc}/{lang}/plain_sentences.gz'.format(data=data, pproc=PPROC, lang=LANG1),
        plain2='{data}/preprocess/{{target}}/{pproc}/{lang}/plain_sentences.gz'.format(data=data, pproc=PPROC, lang=LANG2),
        url1='{data}/preprocess/{{target}}/{pproc}/{lang}/url.gz'.format(data=data, pproc=PPROC, lang=LANG1),
        url2='{data}/preprocess/{{target}}/{pproc}/{lang}/url.gz'.format(data=data, pproc=PPROC, lang=LANG2),
        tok1='{data}/preprocess/{{target}}/{pproc}/{lang}/plain_tokenized.gz'.format(data=data, pproc=PPROC, lang=LANG1),
        tok2='{data}/preprocess/{{target}}/{pproc}/{lang}/plain_tokenized.gz'.format(data=data, pproc=PPROC, lang=LANG2),
    output: '{transient}/{{target}}/hunalign.docalign.{{docalignext}}.xz'.format(transient=transient)
    params:
        c1=lambda wildcards: 1 if wildcards.docalignext == 'bitextor' else 2,
        c2=lambda wildcards: 2 if wildcards.docalignext == 'bitextor' else 3
    shell:
        '''
        cut -f {params.c1},{params.c2} {input.indices} |
        LC_ALL=C sort -nk1 |
        {PROFILING} python3 {BITEXTOR}/bitextor-build-docalign.py --columns1 {input.url1} {input.plain1} {input.tok1} --columns2 {input.url2} {input.plain2} {input.tok2} |
        awk -F\'\t\' \'{{print $2,$6,$3,$7,$4,$8}}\' OFS=\'\t\' |
        xz -T 0 -f > {output} # Format: url1 <tab> url2 <tab> text1 <tab> text2 <tab> tok1 <tab> tok2
        '''

rule hunaligndic:
    input:
        expand("{dic}", dic=DIC)
    output:
        '{dir}/hunalign_dic'.format(dir=transient)
    run:
        with open(output[0], "wt") as outw:
            with open(input[0], "rt") as inr:
                header=inr.readline().strip()
                langs=header.split("\t")
                if langs[0] == LANG1 and langs[1] == LANG2:
                    inverse=True
                else:
                    inverse=False
                for inline in inr:
                    columns=inline.strip().split("\t")
                    if inverse:
                        outw.write(columns[1]+" @ "+columns[0]+"\n")
                    else:
                        outw.write(columns[0]+" @ "+columns[1]+"\n")

rule alignsegments_hunalign:
    input:
        '{transientdir}/hunalign_dic'.format(transientdir=transient, extension=DOCALIGNEXT),
        "{transient}/{{target}}/hunalign.docalign.{extension}.xz".format(transient=transient,extension=DOCALIGNEXT)
    output:
        '{transient}/{{target}}/hunalign.segalign.xz'.format(transient=transient)
    shell:
        'xzcat -T 0 {input[1]} | {PROFILING} {BITEXTOR}/bitextor-align-segments.py -d {input[0]} -t {TMPDIR} --hunalign-dir "{BITEXTOR}/hunalign/src/hunalign" | xz -T 0 > {output};'

##################################################

rule deferred_documents:
    input:
        "{data}/preprocess/{{target}}/{pproc}/{{lang}}/normalized_html.gz".format(data=data, pproc=PPROC),
        "{data}/preprocess/{{target}}/{pproc}/{{lang}}/url.gz".format(data=data, pproc=PPROC)
    output:
        "{data}/preprocess/{{target}}/{pproc}/{{lang}}/html5lib_plain_text.xz".format(data=data, pproc=PPROC),
        "{data}/preprocess/{{target}}/{pproc}/{{lang}}/deferred_documents.xz".format(data=data, pproc=PPROC)
    shell:
        'touch {output[0]}.touch; xz {output[0]}.touch; mv {output[0]}.touch.xz {output[0]}; touch {output[1]}.touch; xz {output[1]}.touch; mv {output[1]}.touch.xz {output[1]}; paste <(zcat {input[0]}) <(zcat {input[1]}) | {PROFILING} python3 {BITEXTOR}/standoff/deferred-document.py | awk \'{{ print $1 | "xz > {output[0]}"; print $3 | "xz > {output[1]}" }}\''

rule deferred_segments:
    input:
        "{transient}/{{target}}/{segaligner}.segalign.xz".format(segaligner=SEGMENTALIGNER, transient=transient),
        "{data}/preprocess/{{target}}/{pproc}/{LANG1}/html5lib_plain_text.xz".format(data=data, pproc=PPROC,LANG1=LANG1REVERSE),
        "{data}/preprocess/{{target}}/{pproc}/{LANG1}/url.gz".format(data=data, pproc=PPROC,LANG1=LANG1REVERSE),
        "{data}/preprocess/{{target}}/{pproc}/{LANG1}/deferred_documents.xz".format(data=data, pproc=PPROC,LANG1=LANG1REVERSE),
        "{data}/preprocess/{{target}}/{pproc}/{LANG2}/html5lib_plain_text.xz".format(data=data, pproc=PPROC,LANG2=LANG2REVERSE),
        "{data}/preprocess/{{target}}/{pproc}/{LANG2}/url.gz".format(data=data, pproc=PPROC,LANG2=LANG2REVERSE),
        "{data}/preprocess/{{target}}/{pproc}/{LANG2}/deferred_documents.xz".format(data=data, pproc=PPROC,LANG2=LANG2REVERSE)
    output:
        "{transient}/{{target}}/{segaligner}.deferred.segalign.xz".format(transient=transient,segaligner=SEGMENTALIGNER)
    shell:
        'xzcat -T 0 -f {input[0]} | {PROFILING} python3 {BITEXTOR}/standoff/deferred-sentences.py <(paste <(xzcat {input[1]} {input[4]}) <(zcat {input[2]} {input[5]}) <(xzcat {input[3]} {input[6]})) > {output}'

rule cleansegments:
    input:
        "{dir}/"+"{segaligner}{deferredsents}.segalign.xz".format(segaligner=SEGMENTALIGNER, deferredsents=DEFERREDSENTENCES)
    output:
        "{dir}/"+"{segaligner}.segclean.xz".format(segaligner=SEGMENTALIGNER)
    shell:
        'xzcat -T 0 -f {input} | {PROFILING} {BITEXTOR}/bitextor-cleantextalign.py -q {MINQUALITY} -m {MAXLINES} -s | xz -T 0 > {output}'

#================================== POST PROCESSING ==================================#

rule bifixer:
    input:
        "{prefix}.segclean.xz"
    output:
        "{prefix}.bifixer.xz"
    shell:
        '''
        xzcat -T 0 -f {input} |
        {PROFILING} python3 {BITEXTOR}/bifixer/bifixer/bifixer.py -q - - {LANG1REVERSE} {LANG2REVERSE} {BIFIXEROPTIONS} |
        awk '{{ print $(NF-1)\"\t\"$NF\"\t\"$0 }}' |
        LC_ALL=C sort -t $'\t' -k1,1 -k2,2nr -T {TMPDIR} --compress-program=gzip -n -r |
        cut -f 3- | xz -T 0 > {output}
        '''

rule bicleaner:
    input:
        segclean="{{prefix}}.{extension}.xz".format(extension=BIFIXER),
        model="{model}".format(model=BICLEANER_CONFIG)
    output:
        "{prefix}.bicleaner.scores.xz"
    shell:
        '''
        slang=$(egrep "source_lang" {BICLEANER_CONFIG} | cut -d " " -f 2)
        if [ "$slang" == "{LANG1REVERSE}" ]; then
          xzcat -T 0 -f {input.segclean} |
          {PROFILING} {BITEXTOR}/preprocess/bin/cache {CACHEOPTIONS} python3 {BITEXTOR}/bicleaner/bicleaner/bicleaner_classifier_lite.py --score_only -q - - {BICLEANER_CONFIG} |
          paste <(xzcat -T 0 {input.segclean}) - |
          xz -T 0 > {output}
        else
          xzcat -T 0 -f {input.segclean} |
          awk \' BEGIN {{FS="\t"; OFS="\t"}} {{ t = $3; $3 = $4; $4 = t; print;}}\' |
          {PROFILING} {BITEXTOR}/preprocess/bin/cache {CACHEOPTIONS} python3  {BITEXTOR}/bicleaner/bicleaner/bicleaner_classifier_lite.py --score_only -q - - {BICLEANER_CONFIG} |
          paste <(xzcat -T 0 {input.segclean}) - |
          xz -T 0 > {output}
        fi
        '''

rule bicleanerfilter:
    input:
        "{prefix}.bicleaner.scores.xz"
    output:
        "{prefix}.bicleaner.xz"
    shell:
        'xzcat -T 0 -f {input} | {PROFILING} {BITEXTOR}/bitextor-filterbicleaner.py --threshold {BICLEANER_THRESHOLD} | xz -T 0 > {output}'

rule elrc:
    input:
        "{{prefix}}.{extension}.xz".format(extension=BICLEANER)
    output:
        "{prefix}.elrc.xz"
    shell:
        'xzcat -T 0 -f {input} | {PROFILING} {BITEXTOR}/bitextor-elrc-filtering.py -c "url1,url2,seg1,seg2,hunalign{DEFERREDFIELDS}{BIFIXERFIELD}{BICLEANEROPTION}" -s | xz -T 0 > {output}'

rule raw:
    input:
        expand("{dir}/{webdomain}/{aligner}.{rawopt}.xz", dir=config["transientDir"], webdomain=domainKey2Hosts.keys(), aligner=SEGMENTALIGNER, rawopt=RAWOPTION)
    output:
        "{permanentdir}/{{l1}}-{{l2}}.raw.xz".format(permanentdir=permanent)
    run:
        #Original command ("xzcat -T 0 {input} | xz -T 0 > {output}") replaced to be able to deal with any number of inputs
        with open(output[0],'wb') as wfd:
            for f in input:
                with open(f,'rb') as fd:
                    shutil.copyfileobj(fd, wfd, 1024*1024*10)

rule sents:
    input:
        expand("{dir}/{webdomain}/{aligner}.{ext}.xz", dir=config["transientDir"], webdomain=domainKey2Hosts.keys(), aligner=SEGMENTALIGNER, ext={ELRCSCORES})
    output:
        "{permanentdir}/{{l1}}-{{l2}}.sent.xz".format(permanentdir=permanent)
    run:
        #Original command ("xzcat -T 0 {input} | xz -T 0 > {output}") replaced to be able to deal with any number of inputs
        with open(output[0],'wb') as wfd:
            for f in input:
                with open(f,'rb') as fd:
                    shutil.copyfileobj(fd, wfd, 1024*1024*10)

rule tmx:
    input:
        "{permanentdir}/{{l1}}-{{l2}}.sent.xz".format(permanentdir=permanent)
    output:
        "{permanentdir}/{{l1}}-{{l2}}.not-deduped.tmx.xz".format(permanentdir=permanent)
    shell:
        "xzcat -T 0 -f {input} | {PROFILING} {BITEXTOR}/bitextor-buildTMX.py --lang1 {LANG1REVERSE} --lang2 {LANG2REVERSE} -c url1,url2,seg1,seg2,hunalign{DEFERREDFIELDS}{BIFIXERFIELD}{BICLEANEROPTION}{ELRCFIELDS} | xz -T 0 > {output}"

rule deduped_tmx:
    input:
        "{permanentdir}/{{l1}}-{{l2}}.sent.xz".format(permanentdir=permanent)
    output:
        tmx="{permanentdir}/{{l1}}-{{l2}}.deduped.tmx.xz".format(permanentdir=permanent),
        txt="{permanentdir}/{{l1}}-{{l2}}.deduped.txt.xz".format(permanentdir=permanent)
    shell:
        "xzcat -T 0 -f {input} | {BICLEANER_SORT} {PROFILING} {BITEXTOR}/bitextor-buildTMX.py --lang1 {LANG1REVERSE} --lang2 {LANG2REVERSE} -c url1,url2,seg1,seg2,hunalign{DEFERREDFIELDS}{BIFIXERFIELD}{BICLEANEROPTION}{ELRCFIELDS} {DEDUP} -f {output.txt} | xz -T 0 > {output.tmx}"

rule mt_parallel_data:
    input:
         "{permanentdir}/{l1}-{l2}.sent.xz".format(permanentdir=permanent, l1=LANG1REVERSE, l2=LANG2REVERSE)
    output:
         l1="{permanentdir}/crawl.{lang}".format(permanentdir=permanent, lang=LANG1REVERSE)
         ,
         l2="{permanentdir}/crawl.{lang}".format(permanentdir=permanent, lang=LANG2REVERSE)
    shell:
         "xzcat -T 0 -f {input} | cut -f 3,4 | LC_ALL=C sort -T {TMPDIR} --compress-program=gzip | uniq > corpus; "
         "cut -f 1 corpus > {output.l1} && cut -f 2 corpus > {output.l2}; "
         "rm corpus"

#################### TRAIN BILINGUAL DICTIONARIES #############################

#Temporal directories for generated data
preprocCorpusDir=transient+"/tempcorpuspreproc."+LANG1+"-"+LANG2
mgizaModelDir=transient+"/tempgizamodel."+LANG1+"-"+LANG2

#Input data prefixes
if "initCorpusTrainPrefix" in config:
    trainPrefixes=config["initCorpusTrainPrefix"]
else:
    trainPrefixes=None

#Obtaining the harmonic probability of each pair of words in both directions and filtering out those with less than p=0.2; printing the dictionary
rule symmetrise_dic:
    input:
        vcb1="{dir}/corpus.{l1}.filtered.vcb".format(dir=mgizaModelDir, l1=LANG1),
        vcb2="{dir}/corpus.{l2}.filtered.vcb".format(dir=mgizaModelDir, l2=LANG2),
        t3_1="{dir}/corpus.{l1}-{l2}.t3.final".format(dir=mgizaModelDir, l1=LANG1, l2=LANG2),
        t3_2="{dir}/corpus.{l2}-{l1}.t3.final".format(dir=mgizaModelDir, l1=LANG1, l2=LANG2)
    output:
        "{file}".format(file=DIC)
    run:
        svocabulary={}
        tvocabulary={}
        svcb=open(input.vcb1,"r")
        tvcb=open(input.vcb2,"r")
        for line in svcb:
            item=line.strip().split(" ")
            svocabulary[item[0]]=item[1]

        for line in tvcb:
            item=line.strip().split(" ")
            tvocabulary[item[0]]=item[1]

        t3dic={}
        t3s=open(input.t3_1,"r")
        t3t=open(input.t3_2,"r")
        for line in t3t:
            item=line.strip().split(" ")
            if item[1] in t3dic:
                t3dic[item[1]][item[0]]=item[2]
            else:
                t3dic[item[1]]={}
                t3dic[item[1]][item[0]]=item[2]

        dic=open(output[0], "wt")
        dic.write(LANG1+"\t"+LANG2+"\n")
        for line in t3s:
            item=line.strip().split(" ")
            if item[0] in t3dic:
                if item[1] in t3dic[item[0]]:
                    value1=float(t3dic[item[0]][item[1]])
                    value2=float(item[2])
                    hmean=2/((1/value1)+(1/value2))

                    if hmean > 0.1:
                        if item[1] in svocabulary and item[0] in tvocabulary:
                            word1=svocabulary[item[1]]
                            word2=tvocabulary[item[0]]
                            if word1.isalpha() or word2.isalpha():
                                dic.write("{0}\t{1}\n".format(word1, word2))
        svcb.close()
        tvcb.close()
        t3s.close()
        t3t.close()
        dic.close()
        os.sync()

#Obtaining the harmonic probability of each pair of words in both directions and filtering out those with less than p=0.2; printing the dictionary
rule lex_dic:
    input:
        vcb1="{dir}/corpus.{l1}.filtered.vcb".format(dir=mgizaModelDir, l1=LANG1),
        vcb2="{dir}/corpus.{l2}.filtered.vcb".format(dir=mgizaModelDir, l2=LANG2),
        t3_1="{dir}/corpus.{l1}-{l2}.t3.final".format(dir=mgizaModelDir, l1=LANG1, l2=LANG2),
        t3_2="{dir}/corpus.{l2}-{l1}.t3.final".format(dir=mgizaModelDir, l1=LANG1, l2=LANG2)
    output:
        e2f="{file}.lex.e2f.gz".format(file=DIC),
        f2e="{file}.lex.f2e.gz".format(file=DIC)
    run:
        svocabulary={}
        tvocabulary={}
        svcb=open(input.vcb1,"r")
        tvcb=open(input.vcb2,"r")
        for line in svcb:
            item=line.strip().split(" ")
            svocabulary[item[0]]=item[1]

        for line in tvcb:
            item=line.strip().split(" ")
            tvocabulary[item[0]]=item[1]

        t3s=open(input.t3_1,"r")
        t3t=open(input.t3_2,"r")
        dice2f=gzip.open(output[0], "wt")
        dicf2e=gzip.open(output[1], "wt")

        for line in t3t:
            item=line.strip().split(" ")
            value = float(item[2])
            if value > 0.1:
                if item[0] in svocabulary and item[1] in tvocabulary:
                    dice2f.write("{0} {1} {2}\n".format(svocabulary[item[0]],tvocabulary[item[1]],item[2]))

        for line in t3s:
            item=line.strip().split(" ")
            value = float(item[2])
            if value > 0.1:
                if item[1] in svocabulary and item[0] in tvocabulary:
                    dicf2e.write("{0} {1} {2}\n".format(tvocabulary[item[0]],svocabulary[item[1]],item[2]))
        svcb.close()
        tvcb.close()
        t3s.close()
        t3t.close()
        dice2f.close()
        dicf2e.close()
        os.sync()

rule filter_dics:
    input:
        "{prefix}.vcb"
    output:
        "{prefix}.filtered.vcb"
    shell:
        "cat {input} | egrep ' [^ ][^ ]+$' > {output}"

rule mgiza:
    input:
        vcb1="{prefix}.{l1}.vcb",
        vcb2="{prefix}.{l2}.vcb",
        snt="{prefix}.{l2}-{l1}-int-train.snt",
        cooc="{prefix}.{l2}-{l1}.cooc"
    output:
        "{prefix}.{l2}-{l1}.t3.final"
    shell:
        "{PROFILING} {BITEXTOR}/mgiza/mgizapp/bin/mgiza -ncpus 8 -CoocurrenceFile {input.cooc} -c {input.snt} -m1 5 -m2 0 -m3 3 -m4 3 -mh 5 -m5 0 -model1dumpfrequency 1 -o {wildcards.prefix}.{wildcards.l2}-{wildcards.l1} -s {input.vcb1} -t {input.vcb2} -emprobforempty 0.0 -probsmooth 1e-7 2> /dev/null > /dev/null"


rule snt2cooc:
    input:
        vcb1="{prefix}.{l1}.vcb",
        vcb2="{prefix}.{l2}.vcb",
        vcb1cls="{prefix}.{l1}.vcb.classes",
        vcb2cls="{prefix}.{l2}.vcb.classes",
        snt="{prefix}.{l2}-{l1}-int-train.snt"
    output:
        "{prefix}.{l2}-{l1}.cooc"
    shell:
        "{PROFILING} {BITEXTOR}/mgiza/mgizapp/bin/snt2cooc {output} {input.vcb1} {input.vcb2} {input.snt} 2> /dev/null"

rule mkcls:
    input:
        "{dir}/corpus.clean.{{lang}}".format(dir=preprocCorpusDir)
    output:
        "{dir}/corpus.{{lang}}.vcb.classes".format(dir=mgizaModelDir)
    priority: 40

    shell:
        "{PROFILING} {BITEXTOR}{MKCLS} -c50 -n2 -p{input} -V{output} opt 2> /dev/null > /dev/null"

rule plain2snt:
    input:
        l1="{dir}/corpus.clean.{l1}".format(dir=preprocCorpusDir, l1=LANG1),
        l2="{dir}/corpus.clean.{l2}".format(dir=preprocCorpusDir, l2=LANG2)
    output:
        snt_2_1="{dir}/corpus.{l2}-{l1}-int-train.snt".format(dir=mgizaModelDir, l1=LANG1, l2=LANG2),
        snt_1_2="{dir}/corpus.{l1}-{l2}-int-train.snt".format(dir=mgizaModelDir, l1=LANG1, l2=LANG2),
        vcb1="{dir}/corpus.{l1}.vcb".format(dir=mgizaModelDir, l1=LANG1),
        vcb2="{dir}/corpus.{l2}.vcb".format(dir=mgizaModelDir, l2=LANG2)
    priority: 40

    shell:
        "mkdir -p {mgizaModelDir}; "
        "{BITEXTOR}/mgiza/mgizapp/bin/plain2snt {input.l1} {input.l2} 2> /dev/null > /dev/null; "
        "mv {preprocCorpusDir}/corpus.clean.{LANG1}_corpus.clean.{LANG2}.snt {output.snt_2_1}; "
        "mv {preprocCorpusDir}/corpus.clean.{LANG2}_corpus.clean.{LANG1}.snt {output.snt_1_2}; "
        "cp {preprocCorpusDir}/corpus.clean.{LANG1}.vcb {output.vcb1}; "
        "cp {preprocCorpusDir}/corpus.clean.{LANG2}.vcb {output.vcb2}; "

#Clean corpus

rule clean:
    input:
        "{prefix}.tok.low."+"{lang1}".format(lang1=LANG1)
        ,
        "{prefix}.tok.low."+"{lang2}".format(lang2=LANG2)
    output:
        "{prefix}.clean."+"{lang1}".format(lang1=LANG1)
        ,
        "{prefix}.clean."+"{lang2}".format(lang2=LANG2)
    shell:
        "{PROFILING} perl {BITEXTOR}/utils/clean-corpus-n.perl {wildcards.prefix}.tok.low {LANG1} {LANG2} {wildcards.prefix}.clean 1 80 {wildcards.prefix}.lines-retained"

#Lowercase corpus

rule lowercase:
    input:
        "{prefix}.tok.{lang}.xz"
    output:
        "{prefix}.tok.low.{lang}"
    shell:
        "xzcat {input} | {PROFILING} {BITEXTOR}/preprocess/moses/tokenizer/lowercase.perl > {output}"

#Tokenize corpus

rule tokenize_file_l1:
    input:
         expand("{dataset}.{lang}.xz", dataset=trainPrefixes, lang=LANG1)
    output:
        "{preprocCorpusDir}/corpus.tok."+"{lang}.xz".format(lang=LANG1)
    shell:
        "mkdir -p {preprocCorpusDir}; "
        "xzcat -T 0 -f {input} | sed \"s/&apos;/'/g\" | sed 's/&quot;/\"/g' | sed 's/&amp;/\&/g' | {WORDTOK1} | xz -T 0 > {output}"

rule tokenize_file_l2:
    input:
         expand("{dataset}.{lang}.xz", dataset=trainPrefixes, lang=LANG2)
    output:
        "{preprocCorpusDir}/corpus.tok."+"{lang}.xz".format(lang=LANG2)
    shell:
        "mkdir -p {preprocCorpusDir}; "
        "xzcat -T 0 -f {input} | sed \"s/&apos;/'/g\" | sed 's/&quot;/\"/g' | sed 's/&amp;/\&/g' | {WORDTOK2} | xz -T 0 > {output}"

if "bicleanerCorpusTrainingPrefix" in config:
    bicleanerTrainPrefixes=config["bicleanerCorpusTrainingPrefix"]
else:
    bicleanerTrainPrefixes=None

rule bicleaner_train_model:
    input:
        corpusl1=expand("{dataset}.{lang}.xz", dataset=bicleanerTrainPrefixes, lang=LANG1),
        corpusl2=expand("{dataset}.{lang}.xz", dataset=bicleanerTrainPrefixes, lang=LANG2),
        e2f="{file}.lex.e2f.gz".format(file=DIC),
        f2e="{file}.lex.f2e.gz".format(file=DIC)
    output:
        "{model}".format(model=BICLEANER_CONFIG)

    shell:
        "training=$(mktemp {TMPDIR}/train.XXXXXXXX); "
        "paste <(xzcat -f {input.corpusl1}) <(xzcat -f {input.corpusl2}) > $training; "
        "DIR=$(dirname {BICLEANER_CONFIG}); "
        "echo $DIR; "
        "lines=$(cat $training | wc -l); "
        "trainlines=$(echo \"$lines*4/10\" | bc); "
        "testlines=$(echo \"($lines-2*$trainlines)/2\" | bc); "
        '{PROFILING} python3  {BITEXTOR}/bicleaner/bicleaner/bicleaner_train.py $training -S "{WORDTOK1}" -T "{WORDTOK2}" --treat_oovs --normalize_by_length -s {LANG1} -t {LANG2} -d {input.e2f} -D {input.f2e} -c $DIR/{LANG1}-{LANG2}.classifier -g $trainlines -w $trainlines --good_test_examples $testlines --wrong_test_examples $testlines -m {BICLEANER_CONFIG} --classifier_type random_forest; '
        "rm $training"
